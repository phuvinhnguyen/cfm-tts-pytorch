{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/phuvinhnguyen/cfm-tts-pytorch.git\n",
    "# !pip install vocos einops openai-whisper jiwer scipy\n",
    "\n",
    "from pathlib import Path\n",
    "import torchaudio, torch\n",
    "from einops import rearrange\n",
    "from cfm_tts_pytorch import E2TTS\n",
    "from vocos import Vocos\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login(\"\")\n",
    "\n",
    "\n",
    "e2tts = E2TTS(\n",
    "    cond_drop_prob = 0.25,\n",
    "    transformer = dict(\n",
    "        dim = 64,\n",
    "        depth = 2,\n",
    "        heads = 2,\n",
    "    ),\n",
    "    mel_spec_kwargs = dict(\n",
    "        filter_length = 1024,\n",
    "        hop_length = 256,\n",
    "        win_length = 1024,\n",
    "        n_mel_channels = 100,\n",
    "        sampling_rate = 24000,\n",
    "    )\n",
    ")\n",
    "\n",
    "# count the number of parameters\n",
    "sum(p.numel() for p in e2tts.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from cfm_tts_pytorch.trainer import (\n",
    "    HFDataset,\n",
    "    E2Trainer\n",
    ")\n",
    "\n",
    "e2tts = E2TTS(\n",
    "    cond_drop_prob = 0.25,\n",
    "    transformer = dict(\n",
    "        dim = 64,\n",
    "        depth = 2,\n",
    "        heads = 2,\n",
    "    ),\n",
    "    mel_spec_kwargs = dict(\n",
    "        filter_length = 1024,\n",
    "        hop_length = 256,\n",
    "        win_length = 1024,\n",
    "        n_mel_channels = 100,\n",
    "        sampling_rate = 24000,\n",
    "    )\n",
    ")\n",
    "\n",
    "train_dataset = HFDataset(load_dataset(\"hishab/MushanWGLOBEEval\", split='test[:5]'))\n",
    "\n",
    "trainer = E2Trainer(\n",
    "    e2tts,\n",
    "    num_warmup_steps=20000,\n",
    "    grad_accumulation_steps = 1,\n",
    "    checkpoint_path = 'e2tts.pt',\n",
    "    log_file = 'e2tts.txt'\n",
    ")\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 1\n",
    "\n",
    "trainer.train(train_dataset, epochs, batch_size, save_step=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocos = Vocos.from_pretrained(\"charactr/vocos-mel-24khz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2tts = E2TTS(\n",
    "    cond_drop_prob = 0.25,\n",
    "    transformer = dict(\n",
    "        dim = 64,\n",
    "        depth = 2,\n",
    "        heads = 2,\n",
    "    ),\n",
    "    mel_spec_kwargs = dict(\n",
    "        filter_length = 1024,\n",
    "        hop_length = 256,\n",
    "        win_length = 1024,\n",
    "        n_mel_channels = 100,\n",
    "        sampling_rate = 24000,\n",
    "    )\n",
    ")\n",
    "\n",
    "checkpoint = torch.load('e2tts.pt', map_location='cpu')\n",
    "e2tts.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-10 10:08:08.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcfm_tts_pytorch.trainer\u001b[0m:\u001b[36m__getitem__\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mAudio shape: (96000,)\u001b[0m\n",
      "/home/kat/.conda/envs/viai/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\u001b[32m2025-05-10 10:08:14.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcfm_tts_pytorch.trainer\u001b[0m:\u001b[36m__getitem__\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mAudio shape: (117600,)\u001b[0m\n",
      "\u001b[32m2025-05-10 10:08:21.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcfm_tts_pytorch.trainer\u001b[0m:\u001b[36m__getitem__\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mAudio shape: (55200,)\u001b[0m\n",
      "\u001b[32m2025-05-10 10:08:25.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcfm_tts_pytorch.trainer\u001b[0m:\u001b[36m__getitem__\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mAudio shape: (76800,)\u001b[0m\n",
      "\u001b[32m2025-05-10 10:08:28.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcfm_tts_pytorch.trainer\u001b[0m:\u001b[36m__getitem__\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mAudio shape: (168001,)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch import cosine_similarity\n",
    "import whisper\n",
    "import jiwer, json, os, torch\n",
    "import numpy as np\n",
    "from scipy.signal import correlate2d\n",
    "from scipy.io.wavfile import write\n",
    "from cfm_tts_pytorch.trainer import HFDataset, collate_fn\n",
    "\n",
    "\n",
    "# Get asr model\n",
    "asr_model = whisper.load_model(\"small\")\n",
    "\n",
    "save_path = \"./MushanWGLOBEEval\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "test_dataset = load_dataset(\"hishab/MushanWGLOBEEval\", split='test[:5]')\n",
    "dataset_load = HFDataset(test_dataset)\n",
    "\n",
    "# define mfcc\n",
    "mfcc_transform = torchaudio.transforms.MFCC(\n",
    "    sample_rate=24000,\n",
    "    n_mfcc=13,\n",
    "    melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 23, \"center\": False},\n",
    ")\n",
    "\n",
    "wers = []\n",
    "mfccs = []\n",
    "spec_corrs = []\n",
    "for index, (sample, sample_load) in enumerate(zip(test_dataset, dataset_load)):\n",
    "    int16_array = np.int16(sample['audio']['array'] * 32767)\n",
    "    write(f\"{save_path}/test_dataset_{index}.wav\", sample['audio']['sampling_rate'], int16_array)\n",
    "\n",
    "    with open(f\"{save_path}/test_dataset_{index}.txt\", \"w\") as f:\n",
    "        f.write(sample['transcript'])\n",
    "    \n",
    "    audio, sr = torchaudio.load(Path(f\"{save_path}/test_dataset_{index}.wav\").expanduser())\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        duration = sample_load['mel_spec'].shape[1] * 2\n",
    "        generated = e2tts.sample(\n",
    "            cond = audio,\n",
    "            text = [sample_load['text'] + ' ' + sample_load['text']],\n",
    "            duration = duration,\n",
    "            steps = 16,\n",
    "            cfg_strength = 0.5,\n",
    "            save_to_filename=f\"{save_path}/test_dataset_{index}_generated.wav\"\n",
    "        )\n",
    "    \n",
    "    # Load and preprocess audio for Whisper\n",
    "    audio, sr = torchaudio.load(f\"{save_path}/1.test_dataset_{index}_generated.wav\")\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        audio = resampler(audio)\n",
    "    audio = audio.squeeze().numpy()\n",
    "\n",
    "    # Whisper expects audio length in seconds (not normalized)\n",
    "    transcription = asr_model.transcribe(audio, language='en')[\"text\"].strip()\n",
    "    \n",
    "    # Compute WER\n",
    "    wer = jiwer.wer(sample['transcript'], transcription)\n",
    "    wers.append(wer)\n",
    "\n",
    "    # # Compute MFCC + Cosine Similarity\n",
    "    # mfcc = mfcc_transform(audio)\n",
    "    # mfcc_ref = mfcc_transform(sample['audio'])\n",
    "    # mfccs.append(cosine_similarity(mfcc, mfcc_ref))\n",
    "    \n",
    "    # # Spectrogram Cross-Correlation\n",
    "    # spec1 = torchaudio.transforms.Spectrogram()(audio).squeeze().numpy()\n",
    "    # spec2 = torchaudio.transforms.Spectrogram()(sample['audio']).squeeze().numpy()\n",
    "    # corr = correlate2d(spec1, spec2, mode='valid')\n",
    "    # similarity_score = np.max(corr)\n",
    "    # spec_corrs.append(similarity_score)\n",
    "\n",
    "    # print(wers[-1], mfccs[-1], spec_corrs[-1])\n",
    "\n",
    "with open(f\"{save_path}/test_dataset_evaluation.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"wer\": wer,\n",
    "        \"mfcc\": mfccs,\n",
    "        \"spec_corr\": spec_corrs\n",
    "    }, f)  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "viai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
